\documentclass{article}
\usepackage{polski}
\usepackage{blindtext}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx} 
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{float}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage[a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm, headsep=1.2cm]{geometry}

\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}


\begin{document}
\title{\textsc{Sprawozdanie - laboratorium 5}}
\subtitle{\textbf{Diagonalizacja macierzy metodą potęgową z wykorzystaniem Redukcji Hotellinga}}
\author{Zuzanna Grzesik}
\date{1 kwietnia 2020}

\maketitle	

\section{Wstęp teoretyczny \cite{1} }	
\subsection{Metoda potęgowa wyznaczania pojedynczych wartości własnych i wektorów własnych}
Załóżmy, że istnieje $n$ liniowo niezależnych wektorów własnych macierzy A, stanowią bazę przestrzeni liniowej, oznaczanych dalej jako $\textbf{x}_1, \textbf{x}_2, \cdots \textbf{x}_n$. Wówczas dla dowolnego wektora $v_0$ zachodzi równość:
\begin{equation}
\textbf{v}_0 = \displaystyle\sum_{i=1}^{n} a_i\textbf{x}_i .
\end{equation}
Jeśli $\lambda_i$ stanowią wartości własne macierzy \textbf{A}, to:
\begin{equation}
\textbf{Av}_0 = \displaystyle\sum_{i=1}^{n} a_i \lambda_i \textbf{x}_i ,
\end{equation}
\begin{equation}
\textbf{v}_m = \textbf{A}^m \textbf{v}_0 = \displaystyle\sum_{i=1}^{n} a_i \lambda_i ^m \textbf{x}_i ,
\end{equation}
gdzie zakładamy, że wartości własne tworzą ciąg $|\lambda_1| \geq |\lambda_2| \geq |\lambda_3| \geq \cdots \geq |\lambda_n | $ .
Jeśli $\lambda_1 $ jest dominującą wartością własną (tzn., że zachodzi zależność, że $\frac{\lambda_j}{\lambda_1} < 1$ dla $j \neq 1$) oraz $\textbf{v}_0$  ma składową w kierunku $\textbf{x}_1$ to wówczas zachodzi:
\begin{equation}
\lim\limits_{m \to \infty} \frac{\textbf{A}^m \textbf{v}_0}{\lambda_1 ^m} = a_1 \textbf{x}_1.
\end{equation}
Korzystając z zależności z równania (3) możemy wyliczać wartość własną w następujący sposób:
\begin{equation}
\lambda_1 = \lim\limits_{m \to \infty} \frac{\textbf{y}^T \textbf{v}_{m+1}}{\textbf{y}^T \textbf{v}_m},
\end{equation}
dla dowolnego wektora $\textbf{y}$ nieortogonalnego do $\textbf{x}_1$. 
\par Wektory własne macierzy wyznacza się w następujący sposób. 
\begin{equation}
\textbf{v}_m \approx \lambda_1 ^m a_1 \textbf{x}_1
\end{equation}
więc unormowany wektor własny będzie miał postać:
\begin{equation}
\textbf{x} = \frac{\textbf{v}_m}{|\textbf{v}_m |}
\end{equation}
Jeśli wartość własna jest pierwiastkiem wielokrotnym równania charakterystycznego to metoda jest zbieżna bo składnik z $\lambda_1$ dominuje:
\begin{equation}
\textbf{v}_m = \textbf{A}^m \textbf{v}_0 = \lambda_1 ^m \displaystyle\sum_{i=1}^{k} a_i \textbf{x}_i + \displaystyle\sum_{i=k+1}^{n} \lambda_i ^m a_i \textbf{x}_i .
\end{equation}
\par Powyższa metoda w tej postaci pozwala nam na wyznaczanie tylko pierwszych wartości wektorów własnych. By wyznaczyć kolejne należy skorzystać z jednej z trzech metod, jakimi są:
\begin{itemize}
	\item metoda redukcji wektora,
	\item metoda zerowania składowej,
	\item metoda redukcji macierzy.
\end{itemize}
Metoda, którą należało zaimplementować z trakcie laboratoriów korzystała z ostatniej z wymienionych czyli redukcji macierzy. 
\subsection{Redukcja macierzy, redukcja Hotellinga}
\textbf{Twierdzenie:} Jeżeli $\lambda_1$ jest wartością własną macierzy \textbf{A} i $\textbf{x}_1$ odpowiadającym jej wektorem własnym oraz dla dowolnego wektora \textbf{v} o własności:
\begin{equation}
\textbf{v}^T \textbf{x}_1 = 1,
\end{equation}
to wtedy macierz zredukowana:
\begin{equation}
\textbf{W}_1 = \textbf{A} = \lambda_1 \textbf{x}_1 \textbf{v}^T
\end{equation}
ma te same wartości co macierz \textbf{A}, oprócz $\lambda_1$, która jest zerem.
\par W zadaniu obliczeń dokonywalismy na macierzy symetrycznej, dlatego korzystaliśmy z redukcji Hotellinga. W metodzie tej za wektor \textbf{v} przyjmujemy lewy wektor własny przynależny do wartości własnje $\lambda_1$, jednak zwykle nie znamy ich, dlatego metoda skuteczna jest tylko w przypadku macierzy symetrycznych, gdzie lewe wektory są identyczne z prawymi. Wówczas, gdy $\textbf{v} = \textbf{x}_1$
\begin{equation}
\textbf{W}_1 = \textbf{A} - \lambda_1 \textbf{x}_1 \textbf{x}_1 ^T .
\end{equation}
W postaci rekurencyjnej:
\begin{equation}
\begin{array}{c}
	\textbf{W}_0 = \textbf{A} \\
	\textbf{W}_i = \textbf{W}_{i-1} - \lambda_{i-1} \textbf{x}_{i-1} \textbf{x}_{i-1} ^T \\
	i = 1,2, \cdots, n-1.
\end{array}
\end{equation}

\section{Zadanie do wykonania}
\subsection{Opis problemu}
Głównym zadaniem w trakcie laboratoriów było dokonanie diagonalizacji macierzy \textbf{A} korzystając z metody potęgowej. Macierz \textbf{A} była postaci:
\begin{equation}
A_{ij} = \frac{1}{\sqrt{2 + |i - j|}}
\end{equation}
dla $i,j = 0,1, \cdots , n-1$, dla $n = 7$. 
\par Następnie należało zaimpelementować algorytm służący do wyzanczania wartości wektorów własnych macierzy, metodą iteracyjną, korzystającą z rozkładu Hotellinga. 

\begin{equation}
	\begin{array}{l}
	W_0 = A \\
	for(k=0; k< K_{val}; k++)\{ \\
	\tab \textbf{x}_k ^0 = [1, 1, \cdots , 1] \tab (inicjalizacja wektora startowego)\\
	\tab \tab for(i=1;i<=IT\_MAX; i++) \{ \\
	\tab \tab \tab \textbf{x}_k^{i+1} = W_k \textbf{x}_k ^i \\
	\tab \tab \tab \lambda _k ^i = \frac{(\textbf{x}_k ^{i+1})^T \textbf{x}_k ^i}{(\textbf{x}_k ^{i})^T \textbf{x}_k ^i} \\
	\tab \tab \tab \textbf{x}_k ^i = \frac{\texbf{x}_k ^{i+1}}{\| \textbf{x}_k^{i+1} \| _2} \\
	\tab \tab \} \\
	\tab W_{k+1} = W_k - \lambda_k \textbf{x}_k ^i (\textbf{x}_k ^i)^T     \tab  (iloczyn tensorowy) \\
	\}
	\end{array}
\end{equation}
gdzie $k$ - numer wyznaczanej wartości własnej, $i$ - numer iteracji dla określonego $k$, $A$ - macierz pierwotna, $W_k$ - macierz iteracji, $\lambda_k ^i$ - i-te przybliżenie k-tego wektora własnego, $\textbf{x}_k ^i$ - i-te przybliżenie k-tego wektora własnego, $K_{val} = n$ - liczba wartości własnych do wyznaczania, $IT\_MAX =12$ - maksymalna liczba iteracji dla każdego k. Wyniki pośrednie obliczania wartości własnych należało zapisać do pliku. Na potrzebę powyższego algorytmu stworzyłam następujące funkcje pomocnicze: mnożącą dwie macierze, obliczającą iloczyn skalarny dwóch wektorów, dokonującą transpozycji macierzy oraz obliczającą macierz iteracji. 
\par Następnym zadaniem było wyznaczyć postać macierzy \textbf{D} zdefiniowanej jako iloczyn $\textbf{D} = \textbf{X}^T \textbf{AX}$.
\par Dla wszystkich obliczeń przyjęłam podwójną precyzję (typ \texttt{double}).

\subsection{Wyniki}

Obliczone w zadaniu wektory własne miały postać:
\begin{equation}
\begin{array}{c}
	\textbf{x}_0 = 
	\begin{pmatrix}
	0.352941\\ 0.377935\\0.392221\\0.396889\\0.392221\\0.377935\\0.352941
	\end{pmatrix} 
	\textbf{x}_1 = 
	\begin{pmatrix}
	0.477239\\0.164774\\0.314852\\0.540298\\0.314852\\0.164774\\0.477239
	\end{pmatrix} 
	\textbf{x}_2 = 
	\begin{pmatrix}
	0.364007\\0.461327\\0.138849\\0.518756\\0.141987\\0.466589\\0.358362
	\end{pmatrix}
	\textbf{x}_3 = 
	\begin{pmatrix}
	-0.478726\\-0.447176\\-0.266515\\0.000635757\\0.26617\\0.44604\\0.479611
	\end{pmatrix}\\
	\textbf{x}_4 = 
	\begin{pmatrix}
	0.130626\\-0.338022\\0.476991\\-0.531333\\0.476992\\-0.33802\\0.130628
	\end{pmatrix}
	\textbf{x}_5 = 
	\begin{pmatrix}
	-0.449005\\0.172676\\0.518245\\-6.29403e-09\\-0.518245\\-0.172676\\0.449005
	\end{pmatrix}
	\textbf{x}_6 = 
	\begin{pmatrix}
	0.262282\\-0.520312\\0.400604\\-3.40684e-13\\-0.400604\\0.520312\\-0.262282
	\end{pmatrix}	
\end{array}
\end{equation}
Obliczone wartości własne były równe:
\begin{equation}
	\begin{array}{l}
	\lambda_0 = 3.59586 \\ 
	\lambda_1 = 0.284988\\ 
	\lambda_2 = 0.122786\\ 
	\lambda_3  = 0.590387\\ 
	\lambda_4 = 0.0865954 \\ 
	\lambda_5 = 0.170974\\ 
	\lambda_6 = 0.0981544
	\end{array}
\end{equation}
Kolejne przybliżenia znalezionych wartości własnych $\lambda_k$, dla IT\_MAX = 12 zostały przedstawione na wykresie (Rysunek 1).

\begin{figure}[h]
\includegraphics[width=10cm]{lambda2.eps}
\centering
\caption{Kolejne przybliżenia znalezionych wartości własnych $\lambda_k$ w funkcji numeru iteracji.}
\end{figure}
\newpage
Macierz $\textbf{D}$ miała postać:

\begin{equation}
\textbf{D} = 
	\scriptsize
	\begin{pmatrix}
	\textbf{3.59586} & -1.2268e-13 & -1.27676e-15 & 8.88178e-16 & 1.09357e-14 & 1.77636e-15 & -1.77636e-15 \\
	-1.22548e-13 & \textbf{0.284988} & -6.25256e-06 & -7.6505e-09 & -3.81633e-09 & -1.31839e-16 & 6.93889e-17\\
	-1.26635e-15 & -6.25256e-06 & \textbf{0.122802} & -0.00332763 & -0.000329115 & -3.70643e-12 & -2.25514e-17\\ 
	7.49401e-16 & -7.6505e-09 & -0.00332763 & \textbf{0.590389} & 8.23117e-07 & 1.03251e-14 & 1.38778e-17\\ 
	1.09201e-14 & -3.81633e-09 & -0.000329115 & 8.23117e-07 & \textbf{0.0865957} & -2.97259e-09 & -1.56455e-14\\ 
	1.8735e-15 & -1.38778e-16 & -3.70639e-12 & 1.02071e-14 & -2.97259e-09 & \textbf{0.170974} & -1.03402e-08\\ 
	-2.12504e-15 & 6.76542e-17 & -5.55112e-17 & 2.94903e-17 & -1.56485e-14 & -1.03402e-08 & \textbf{0.0981544} 
	\end{pmatrix}
\end{equation}
\normalsize
Dodatkowo dokonałam obliczeń dla różnych liczb iteracji (IT\_MAX) i wyniki przedstawiłam w tabeli (Tabela 1)\\
\begin{equation*}
\begin{array}{|r|c|c|c|c|c|c|c|}
\hline
\rowcolor{CadetBlue} \textbf{IT\_MAX }& \lambda_0 & \lambda_1 & \lambda_2 & \lambda_3 & \lambda_4 & \lambda_5 & \lambda_6 \\
\hline
\rowcolor{CadetBlue} \textbf{12} & \rowcolor{Grey} 3.59586 & \rowcolor{White} 0.284988 & 0.122786 & 0.590387 & 0.0865954 & 0.170974 & 0.0981544 \\ 
\hline
\rowcolor{CadetBlue}  \textbf{50} & \rowcolor{Grey}3.59586 & \rowcolor{White}  0.590116 & 0.28512 & 0.122787 & 0.170974 & 0.0865947 & 0.0981544 \\
\hline
\rowcolor{CadetBlue} \textbf{100} &\rowcolor{Grey} 3.59586 & 0.59039 &  0.284988 & 0.170683 & \rowcolor{White}  0.122996 & 0.0865947 & 0.0981544 \\
\hline
\rowcolor{CadetBlue}  \textbf{200} & \rowcolor{Grey}  3.59586 & 0.59039 & 0.284988 & 0.170974 & 0.122787 & \rowcolor{White}  0.0865956 & 0.0981534 \\ 
\hline
\rowcolor{CadetBlue}  \textbf{250} & \rowcolor{Grey} 3.59586 &  0.59039 & 0.284988 & 0.170974 & 0.122787 & \rowcolor{White}  0.0976684 & 0.0870256\\
\hline
\rowcolor{CadetBlue}  \textbf{287} & \rowcolor{Grey} 3.59586 & 0.59039 & 0.284988 & 0.170974 & 0.122787 & 0.0981544 & \rowcolor{White} 0.0865948\\
\hline
\rowcolor{CadetBlue}  \textbf{288} & \rowcolor{Grey} 3.59586 &  0.59039 & 0.284988 & 0.170974 & 0.122787 & 0.0981544 & 0.0865947 \\
\hline
\rowcolor{CadetBlue}  \textbf{289} & \rowcolor{Grey} 3.59586 & 0.59039 & 0.284988 & 0.170974 &  0.122787 & 0.0981544 & 0.0865947 \\
\hline
\rowcolor{CadetBlue}  \textbf{300} & \rowcolor{Grey} 3.59586 & 0.59039 & 0.284988 & 0.170974 & 0.122787 & 0.0981544 & 0.0865947\\
\hline
\rowcolor{CadetBlue} \textbf{500} & \rowcolor{Grey} 3.59586 & 0.59039 &  0.284988 & 0.170974 &  0.122787 & 0.0981544 & 0.0865947 \\
\hline
\end{array}
\\
\end{equation*s}
\begin{center} Tabela 1: Końcowe wartości $\lambda_k$ w zależności od wartości IT\_MAX \end{center}



Jak widać powyżej, dokładność przybliżeń wzrasta wraz z liczbą iteracji. Liczba 288 to graniczna liczba, kiedy przybliżone wartości $\lambda_k$ już się nie zmieniają w znacznym stopniu (dla dalszych liczb po przecinku liczby są dalej coraz dokładniejsze). Warto zauważyć też, że obliczone wartości własne dla 300 i więcej iteracji, to wartości obliczone przy mniejszej liczbie iteracji, ale w innej kolejności (czyli np. $\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_5, \lambda_6$ dla IT\_MAX=12 są odpowiednio równe $\lambda_2, \lambda_4, \lambda_1, \lambda_6, \lambda_3, \lambda_5$ dla IT\_MAX=300).
\par Dla dokładniejszych wartości własnych, zmieni się również postać macierzy \textbf{D}.

\section{Wnioski}
Metoda iteracyjna z Redukcją Hotellinga jest w tym przypadku dość szybką i prostą w implementacji metodą znajdowania wartości i wektorów własnych macierzy o wymiarach $7 \times 7$. Jednak, na podstawie wyników, można zauważyć, że dobra dokładność (do 7 miejsca po przecinku) i poprawna kolejność wartości własnych występuje dopiero przy 288 iteracjach. W przypadku małych macierzy, jak ta, na której dokonywaliśmy obliczeń, taka ilość iteracji nie wydłuża w znacznym stopniu czasu obliczeń, ale dla macierzy większego wymiaru metoda ta może być dość wolna. 

\begin{thebibliography}{9}

\bibitem{1}
  Tomasz Chwiej, \emph{Wyznaczanie wartości i wektorów własnych macierzy} 
  \texttt{http://galaxy.agh.edu.pl/~chwiej/mn/diagonalizacja\_2018.pdf}

\end{thebibliography}


\end{document}
